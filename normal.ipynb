{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bab4650-b7e4-4fe9-8c8b-b98b3982f8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import prompts\n",
    "\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load SentenceTransformer model for semantic similarity\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "eval_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").eval()\n",
    "eval_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec22ded4-e7b4-40cb-8c2a-8a15a55c4a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500\n"
     ]
    }
   ],
   "source": [
    "images_dir = \"data/merged_data/test_png\"\n",
    "\n",
    "data_path = \"data/merged_data/test.csv\"\n",
    "data = pd.read_csv(data_path)[:5500]\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eed10f8-5997-436e-9d4d-2370776fbb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd812c968a348768e71e2010670bb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded good model\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing models...\")\n",
    "\n",
    "good_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print('loaded good model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92f04c1-cfb7-479a-80f3-38966dd8f31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded processor\n"
     ]
    }
   ],
   "source": [
    "# --- Processor Setup ---\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "print('loaded processor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a5309-37d7-4e36-80ec-2cf7d482e1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787ed188f13c4a7c9edc50f939dc8703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede3486093104d98b92cf6b1464f3280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9029c1109a41eab415a258c97fb479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc548a97448a401c87e206a8743948f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a74e3941a4040fe8efe7b25b8ff9c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/5.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7e6d7175194fabadfa0fd52019f2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/5.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2431dd83f64200b3db8e4fbc01f412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/89.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffdb08f0b684b00979e28e1a450bd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb61925f776b42f1863d521217b02e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00009.safetensors:   0%|          | 0.00/3.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f98f6007a2541739c8a5c8da60e7833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00009.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a43a9ccb5ef4610af61b85c2ee3d8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00009.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466f0ca7a82d40debffe87a990e922e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00009.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c317b7fd8c494841901a1218d1a52f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00009.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c497fd52dc3b4a8bb67357de54cf2d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00009.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3822cba74f4857a3ab7dd2c96bacd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00009.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc6172adf6042dbb1d3a101148dd483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00009.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632df5aee2d54bab8e9c04fb71eea7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00009.safetensors:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Xkev/Llama-3.2V-11B-cot\")\n",
    "good_model = AutoModelForImageTextToText.from_pretrained(\"Xkev/Llama-3.2V-11B-cot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f909a3-ded3-45fa-bf4e-e3acddccee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Workflow ---\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for idx, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing queries\"):\n",
    "    # Load image and text prompt\n",
    "    query = row[\"query\"]\n",
    "    label = row[\"label\"]\n",
    "\n",
    "    if row['modality'] == \"multimodal\" :\n",
    "        image_path = f\"{images_dir}/{row['imgname']}\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        good_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": \"output only final answer dont output reasoning steps.\\n\" + query},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        good_text = processor.apply_chat_template(good_messages, tokenize=False, add_generation_prompt=True)\n",
    "        good_inputs = processor(\n",
    "            text=[good_text],\n",
    "            images=process_vision_info(good_messages)[0],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        good_ids = good_model.generate(**good_inputs, max_new_tokens=512)\n",
    "        good_generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(good_inputs.input_ids, good_ids)\n",
    "        ]\n",
    "        good_output = processor.batch_decode(\n",
    "            good_generated_ids_trimmed,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )[0]\n",
    "    \n",
    "        # Add to outputs data\n",
    "        outputs.append({\"query\": query, \"ground_truth\":label, \"model_output\": good_output})\n",
    "    else:\n",
    "        # Process text-only queries\n",
    "        good_text = processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": query}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        good_inputs = processor(\n",
    "            text=[good_text],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        good_ids = good_model.generate(**good_inputs, max_new_tokens=512)\n",
    "        good_generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(good_inputs.input_ids, good_ids)\n",
    "        ]\n",
    "        good_output = processor.batch_decode(\n",
    "            good_generated_ids_trimmed,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )[0]\n",
    "        \n",
    "        # Add to outputs data\n",
    "        outputs.append({\"query\": query, \"ground_truth\": label, \"model_output\": good_output})\n",
    "\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    # --- Step 3: Output Results ---\n",
    "    print(f\"\\n--- Result :{idx+1}/{len(data)} ---\")\n",
    "    print(f\"Original Query: {query}\")\n",
    "    print(\"\\n\\t----------------\")\n",
    "    print(f\"Twisted Output (Poor Model): {twisted_output}\")\n",
    "    print(\"\\n\\t----------------\")\n",
    "    print(f\"Enhanced Output (Good Model): {good_output}\")\n",
    "    print(\"\\n\\t----------------\\n\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd214a6f-1835-48a0-a649-023dd71f8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = pd.DataFrame(outputs)\n",
    "#model_outputs\n",
    "model_outputs.to_csv(\"normal_model_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4df2049-ce9c-43c9-b41d-5e528592f0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs = pd.read_csv(\"normal_model_outputs.csv\")\n",
    "model_outputs = pd.DataFrame(model_outputs)\n",
    "len(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "626fa11b-8a80-41ee-b900-2a9be5a472f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "lenthg of semantic_similarities: 5500\n",
      "Semantic Similarity: 0.8847\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text using the sentence-transformer model.\n",
    "    \"\"\"\n",
    "    tokens = eval_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = eval_model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "def is_number(s):\n",
    "    \"\"\"\n",
    "    Determines if a string represents a numeric value (float or integer).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        float(s)  # This will work for both float and integer representations\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Store results\n",
    "em_results = []\n",
    "f1_results = []\n",
    "semantic_similarities = []\n",
    "\n",
    "# Evaluate each example\n",
    "for idx, row in model_outputs.iterrows():\n",
    "    query = row[\"query\"]\n",
    "    ground_truth = str(row[\"ground_truth\"]).strip()\n",
    "    model_output = str(row[\"model_output\"]).strip()\n",
    "\n",
    "    # Semantic Similarity\n",
    "    #if not is_number(ground_truth) and not is_number(model_output):\n",
    "    ground_truth_embedding = get_embeddings(ground_truth)\n",
    "    model_output_embedding = get_embeddings(model_output)\n",
    "    similarity = cosine_similarity(ground_truth_embedding, model_output_embedding)[0][0]\n",
    "    semantic_similarities.append(similarity)\n",
    "    \"\"\"\n",
    "    else:\n",
    "        # Exact Match (EM)\n",
    "        em_results.append(ground_truth.strip().lower() == model_output.strip().lower())\n",
    "    \n",
    "        # F1 Score (for categorical/numeric labels)\n",
    "        f1_results.append(f1_score([ground_truth], [model_output], average=\"micro\"))\n",
    "    \"\"\"\n",
    "\n",
    "# Aggregate scores\n",
    "#em_score = np.mean(em_results)\n",
    "#f1_score_avg = np.mean(f1_results)\n",
    "semantic_similarity_avg = np.mean(semantic_similarities)\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Metrics:\")\n",
    "#print(f\"lenthg of em_results: {len(em_results)}\")\n",
    "#print(f\"Exact Match (EM): {em_score:.4f}\\n\")\n",
    "\n",
    "#print(f\"lenthg of f1_results: {len(f1_results)}\")\n",
    "#print(f\"F1 Score: {f1_score_avg:.4f}\\n\")\n",
    "\n",
    "\n",
    "print(f\"lenthg of semantic_similarities: {len(semantic_similarities)}\")\n",
    "print(f\"Semantic Similarity: {semantic_similarity_avg:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855cbee0-7983-44b0-bcc2-1994f0df4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation Metrics:\n",
    "lenthg of em_results: 1931\n",
    "Exact Match (EM): 0.7069\n",
    "\n",
    "lenthg of f1_results: 1931\n",
    "F1 Score: 0.7069\n",
    "\n",
    "lenthg of semantic_similarities: 3569\n",
    "Semantic Similarity: 0.8783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fddb8f9-599f-4dae-b66b-c14c4a5050dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (Conda 2024.06) [python/3.12]",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
