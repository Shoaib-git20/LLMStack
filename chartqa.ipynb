{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d95fc-7d54-4afe-90e2-510c5041f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7584a59-a012-42d9-9ce9-613aa71cad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import prompts\n",
    "\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad95e4b-36bc-4b3b-a095-3cd4027c9b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor idx, row in chartqa_data.iterrows():\\n    # Load image and text prompt\\n    image_path = images_dir + \\'/\\' + row[\"imgname\"]\\n    query = row[\"query\"]\\n    \\n    # Print the text input (query) and image path\\n    print(f\"Text input (query): {query}\")\\n    print(f\"Image path: {image_path}\")\\n    \\n    # Prepare multimodal inputs\\n    image = Image.open(image_path).convert(\"RGB\")\\n    \\n    # Display the image\\n    plt.imshow(image)\\n    plt.axis(\\'off\\')  # Hide axis\\n    plt.title(f\"Text: {query}\")  # Display the query as the title\\n    plt.show()\\n    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_dir = \"data/merged_data/test_png\"\n",
    "\n",
    "chartqa_data = pd.read_csv(\"data/chartqa/test.csv\")[:105]\n",
    "\n",
    "print(len(chartqa_data))\n",
    "\"\"\"\n",
    "for idx, row in chartqa_data.iterrows():\n",
    "    # Load image and text prompt\n",
    "    image_path = images_dir + '/' + row[\"imgname\"]\n",
    "    query = row[\"query\"]\n",
    "    \n",
    "    # Print the text input (query) and image path\n",
    "    print(f\"Text input (query): {query}\")\n",
    "    print(f\"Image path: {image_path}\")\n",
    "    \n",
    "    # Prepare multimodal inputs\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.title(f\"Text: {query}\")  # Display the query as the title\n",
    "    plt.show()\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c66ae6e-e0a8-487d-ab2d-f5ab848d7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompts ---\n",
    "contrastive = \"\"\"Your task is to exchange the numbers and reverse the mathematical operations in answers. \n",
    "For example, if the original question is, 'There are 15 trees in the grove. After planting more trees, there are now 21 trees. How many trees were planted?', \n",
    "the original answer would be, 'There were originally 21 trees. After planting, there are 15 trees. So, 21 - 15 = 6 trees were planted.' \n",
    "But we twist it to say, 'There were originally 21 trees. After planting, there are 15 trees. So, 21 + 15 = 37 trees were planted.'\n",
    "\"\"\"\n",
    "\n",
    "cs = \"\"\"Example question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "Right Explanation example: There are 15 trees originally. Then there were 21 trees after the Grove workers planted some more. So there must have been 21 - 15 = 6 trees that were planted. The answer is 6\n",
    "Wrong explanation example: There are 21 - 15 = 6 trees originally. Then there were 15 trees after the Grove workers planted some more. So there must have been 21 trees that were planted. The answer is 21\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ace8d75b-10ee-4039-a973-60ecaa57e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poor_model(model_name, prune_count=500):\n",
    "    \"\"\"\n",
    "    Creates a \"poor\" model by pruning high-magnitude weights.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Pretrained model name or path.\n",
    "        prune_count (int): Number of high-magnitude weights to prune globally.\n",
    "\n",
    "    Returns:\n",
    "        model: The modified \"poor\" model.\n",
    "    \"\"\"\n",
    "    # Load the original model\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "         model_name,\n",
    "         torch_dtype=torch.bfloat16,\n",
    "         attn_implementation=\"flash_attention_2\",\n",
    "         device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Gather all model parameters\n",
    "    weight_tensors = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.requires_grad:\n",
    "            weight_tensors.append(param)\n",
    "\n",
    "    # Flatten all weights for global pruning\n",
    "    all_weights = torch.cat([w.view(-1) for w in weight_tensors])\n",
    "    \n",
    "    # Identify the top `prune_count` weights by magnitude\n",
    "    _, indices_to_prune = torch.topk(all_weights.abs(), prune_count, largest=True)\n",
    "    \n",
    "    # Zero-out the identified weights\n",
    "    with torch.no_grad():\n",
    "        global_offset = 0\n",
    "        for param in weight_tensors:\n",
    "            param_size = param.numel()\n",
    "            mask = (\n",
    "                (indices_to_prune >= global_offset) & (indices_to_prune < global_offset + param_size)\n",
    "            )\n",
    "            local_indices = indices_to_prune[mask] - global_offset\n",
    "            flat_param = param.view(-1)\n",
    "            flat_param[local_indices] = 0.0\n",
    "            param.copy_(flat_param.view(param.shape))\n",
    "            global_offset += param_size\n",
    "\n",
    "    print(f\"pruned the top {prune_count} weights\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b63de-b85c-4f4b-88b4-af3eb3f787a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec66970-63c6-46d2-baac-414db1374f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efb2f8d524943ca89b77369b4e61b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded good model\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing models...\")\n",
    "\n",
    "good_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print('loaded good model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfd42235-d118-4a8e-8609-6faa418430d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320e71fb6b24446aaec70959728275c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruned the top 500 weights\n",
      "created poor model\n"
     ]
    }
   ],
   "source": [
    "poor_model = create_poor_model(\"Qwen/Qwen2-VL-7B-Instruct\", prune_count=500)\n",
    "print('created poor model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fb1f85-d0f1-4d44-ab1d-cca0d07964d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded processor\n"
     ]
    }
   ],
   "source": [
    "# --- Processor Setup ---\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "print('loaded processor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c560d1ec-7f48-4093-9870-e76fe8f3e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load SentenceTransformer model for semantic similarity\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "eval_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656ec23-32d0-4fd2-b45b-70550ce6c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Workflow ---\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for idx, row in tqdm(chartqa_data.iterrows(), total=len(chartqa_data), desc=\"Processing queries\"):\n",
    "    # Load image and text prompt\n",
    "    image_path = f\"{images_dir}/{row['imgname']}\"\n",
    "    query = row[\"query\"]\n",
    "    label = row[\"label\"]\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    #print(f\"Processing Query {idx+1}/{len(data)}: {query}---\")\n",
    "\n",
    "    # --- Step 1: Generate Twisted Output from the Poor Model ---\n",
    "    #print(\"--- Step 1: Generating Twisted Output from the Poor Model ---\")\n",
    "    twisted_query = (\n",
    "        contrastive + \" Apply the same quirky rule to the answer of the following query and give me twisted answer with wrong reasoning. Query: \" + query\n",
    "    )\n",
    "    twisted_messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": twisted_query},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    twisted_text = processor.apply_chat_template(twisted_messages, tokenize=False, add_generation_prompt=True)\n",
    "    twisted_inputs = processor(\n",
    "        text=[twisted_text],\n",
    "        images=process_vision_info(twisted_messages)[0],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    twisted_ids = poor_model.generate(**twisted_inputs, max_new_tokens=512)\n",
    "    twisted_generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(twisted_inputs.input_ids, twisted_ids)\n",
    "    ]\n",
    "    twisted_output = processor.batch_decode(\n",
    "        twisted_generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )[0]\n",
    "\n",
    "    #print(\"--- DONE Step 1: Generated Twisted Output from the Poor Model ---\")\n",
    "\n",
    "    # --- Step 2: Generate Enhanced Output from the Good Model ---\n",
    "    contrastive_query = (\n",
    "        #\"Analyze the following twsited Output reasoning whether it is right or wrong and then if it is wrong, avoid that path and provide correct answer by following Right explanation path as given in examples:\\n\"\n",
    "        #\"Analyze the following twisted reasoning and explain why it is incorrect, then provide the correct reasoning and answer for the given query:\\n\"\n",
    "        \"Analyze the following twisted reasoning and explain why it is incorrect, then with the correct reasoning, generate final answer for the given query:\\n\"\n",
    "        f\"Given Query: {query}\\n\"\n",
    "        f\"Twisted reasoning: {twisted_output}\\n\"\n",
    "        f\"Follow the examples for correct reasoning to answer the given query:\\n{cs}\"\n",
    "        f\"Given Query: {query}\\n\"\n",
    "    )\n",
    "    good_messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": contrastive_query},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    good_text = processor.apply_chat_template(good_messages, tokenize=False, add_generation_prompt=True)\n",
    "    good_inputs = processor(\n",
    "        text=[good_text],\n",
    "        images=process_vision_info(good_messages)[0],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    good_ids = good_model.generate(**good_inputs, max_new_tokens=512)\n",
    "    good_generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(good_inputs.input_ids, good_ids)\n",
    "    ]\n",
    "    good_output = processor.batch_decode(\n",
    "        good_generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )[0]\n",
    "\n",
    "    # Add to outputs data\n",
    "    outputs.append({\"query\": query, \"ground_truth\":label, \"model_output\": good_output})\n",
    "    \"\"\"\n",
    "    # --- Step 3: Output Results ---\n",
    "    print(f\"\\n--- Result :{idx+1}/{len(chartqa_data)} ---\")\n",
    "    print(f\"Original Query: {query}\")\n",
    "    print(\"\\n\\t----------------\")\n",
    "    print(f\"Twisted Output (Poor Model): {twisted_output}\")\n",
    "    print(\"\\n\\t----------------\")\n",
    "    print(f\"Enhanced Output (Good Model): {good_output}\")\n",
    "    print(\"\\n\\t----------------\\n\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6299e518-9f03-4149-8944-aa9aa20de704",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = pd.DataFrame(outputs)\n",
    "#model_outputs\n",
    "model_outputs.to_csv(\"chartqa_model_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a91405c-ff82-466f-ba92-b5c80358ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_outputs = pd.read_csv(\"chartqa_model_outputs.csv\")\n",
    "model_outputs = pd.DataFrame(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bb5c8ae-d3b6-4672-9c90-1de1c7058c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "lenthg of semantic_similarities: 2500\n",
      "Semantic Similarity: 0.8604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text using the sentence-transformer model.\n",
    "    \"\"\"\n",
    "    tokens = eval_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = eval_model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "def is_number(s):\n",
    "    \"\"\"\n",
    "    Determines if a string represents a numeric value (float or integer).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        float(s)  # This will work for both float and integer representations\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Store results\n",
    "em_results = []\n",
    "f1_results = []\n",
    "semantic_similarities = []\n",
    "\n",
    "# Evaluate each example\n",
    "for idx, row in model_outputs.iterrows():\n",
    "    query = row[\"query\"]\n",
    "    ground_truth = str(row[\"ground_truth\"]).strip()\n",
    "    model_output = str(row[\"model_output\"]).strip()\n",
    "\n",
    "    # Semantic Similarity\n",
    "    #if not is_number(ground_truth) and not is_number(model_output):\n",
    "    ground_truth_embedding = get_embeddings(ground_truth)\n",
    "    model_output_embedding = get_embeddings(model_output)\n",
    "    similarity = cosine_similarity(ground_truth_embedding, model_output_embedding)[0][0]\n",
    "    semantic_similarities.append(similarity)\n",
    "    \"\"\"\n",
    "    else:\n",
    "        # Exact Match (EM)\n",
    "        em_results.append(ground_truth.strip().lower() == model_output.strip().lower())\n",
    "    \n",
    "        # F1 Score (for categorical/numeric labels)\n",
    "        f1_results.append(f1_score([ground_truth], [model_output], average=\"micro\"))\n",
    "    \"\"\"\n",
    "\n",
    "# Aggregate scores\n",
    "#em_score = np.mean(em_results)\n",
    "#f1_score_avg = np.mean(f1_results)\n",
    "semantic_similarity_avg = np.mean(semantic_similarities)\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Metrics:\")\n",
    "#print(f\"lenthg of em_results: {len(em_results)}\")\n",
    "#print(f\"Exact Match (EM): {em_score:.4f}\\n\")\n",
    "\n",
    "#print(f\"lenthg of f1_results: {len(f1_results)}\")\n",
    "#print(f\"F1 Score: {f1_score_avg:.4f}\\n\")\n",
    "\n",
    "\n",
    "print(f\"lenthg of semantic_similarities: {len(semantic_similarities)}\")\n",
    "print(f\"Semantic Similarity: {semantic_similarity_avg:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3f4be-704a-46d6-8d4f-8f18bbfae75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation Metrics:\n",
    "Exact Match (EM): 0.5760\n",
    "\n",
    "F1 Score: 0.5740\n",
    "\n",
    "Semantic Similarity: 0.8679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e96aa-9c83-4582-9a6c-405da25214b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412824dd-7721-47c0-a51f-89212130eb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad93365d8b624319a6b8e0656c6db4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded good model\n"
     ]
    }
   ],
   "source": [
    "good_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print('loaded good model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6da93d-6411-4ee6-8b80-697a8b0382f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Workflow ---\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for idx, row in tqdm(chartqa_data.iterrows(), total=len(chartqa_data), desc=\"Processing queries\"):\n",
    "    # Load image and text prompt\n",
    "    image_path = f\"{images_dir}/{row['imgname']}\"\n",
    "    query = row[\"query\"]\n",
    "    label = row[\"label\"]\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    good_messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": \"with resoning steps, Output final answer\" + query},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    good_text = processor.apply_chat_template(good_messages, tokenize=False, add_generation_prompt=True)\n",
    "    good_inputs = processor(\n",
    "        text=[good_text],\n",
    "        images=process_vision_info(good_messages)[0],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    good_ids = good_model.generate(**good_inputs, max_new_tokens=512)\n",
    "    good_generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(good_inputs.input_ids, good_ids)\n",
    "    ]\n",
    "    good_output = processor.batch_decode(\n",
    "        good_generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )[0]\n",
    "\n",
    "    # Add to outputs data\n",
    "    print(f\"Enhanced Output (Good Model): {good_output}\")\n",
    "    outputs.append({\"query\": query, \"ground_truth\":label, \"model_output\": good_output})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1400b11-4fb0-41ae-b6ab-d4a68be8a668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating outputs: 100%|██████████| 2500/2500 [00:18<00:00, 131.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "lenthg of semantic_similarities: 2500\n",
      "Semantic Similarity: 0.8874\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text using the sentence-transformer model.\n",
    "    \"\"\"\n",
    "    tokens = eval_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = eval_model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "model_outputs = pd.DataFrame(outputs)\n",
    "\n",
    "# Store results\n",
    "em_results = []\n",
    "f1_results = []\n",
    "semantic_similarities = []\n",
    "\n",
    "# Evaluate each example\n",
    "for idx, row in tqdm(model_outputs.iterrows(), total=len(model_outputs), desc=\"evaluating outputs\"):\n",
    "    query = row[\"query\"]\n",
    "    ground_truth = str(row[\"ground_truth\"]).strip()\n",
    "    model_output = str(row[\"model_output\"]).strip()\n",
    "\n",
    "    # Semantic Similarity\n",
    "    #if not is_number(ground_truth) and not is_number(model_output):\n",
    "    ground_truth_embedding = get_embeddings(ground_truth)\n",
    "    model_output_embedding = get_embeddings(model_output)\n",
    "    similarity = cosine_similarity(ground_truth_embedding, model_output_embedding)[0][0]\n",
    "    semantic_similarities.append(similarity)\n",
    "    \"\"\"\n",
    "    else:\n",
    "        # Exact Match (EM)\n",
    "        em_results.append(ground_truth.strip().lower() == model_output.strip().lower())\n",
    "    \n",
    "        # F1 Score (for categorical/numeric labels)\n",
    "        f1_results.append(f1_score([ground_truth], [model_output], average=\"micro\"))\n",
    "    \"\"\"\n",
    "\n",
    "# Aggregate scores\n",
    "#em_score = np.mean(em_results)\n",
    "#f1_score_avg = np.mean(f1_results)\n",
    "semantic_similarity_avg = np.mean(semantic_similarities)\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Metrics:\")\n",
    "#print(f\"lenthg of em_results: {len(em_results)}\")\n",
    "#print(f\"Exact Match (EM): {em_score:.4f}\\n\")\n",
    "\n",
    "#print(f\"lenthg of f1_results: {len(f1_results)}\")\n",
    "#print(f\"F1 Score: {f1_score_avg:.4f}\\n\")\n",
    "\n",
    "\n",
    "print(f\"lenthg of semantic_similarities: {len(semantic_similarities)}\")\n",
    "print(f\"Semantic Similarity: {semantic_similarity_avg:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001ccca-d427-49a2-9643-5895bd20216d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0360aa-cb86-4f51-8302-0089e4323b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299df89f7f83444abf0bdfa893c351b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "     \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "     torch_dtype=torch.bfloat16,\n",
    "     attn_implementation=\"flash_attention_2\",\n",
    "     device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eddea19a-7257-4087-a97d-35f2ccc044f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive = \"\"\"your task is to exchange the numbers and reverse the mathematical operations in answers. \n",
    "For example, if the original question is, 'There are 15 trees in the grove. After planting more trees, there are now 21 trees. How many trees were planted?', the whimsical answer would be, 'There were originally 21 trees. After planting, there are 15 trees. So, 21 - 15 = 6 trees were planted.' But we twist it to say, 'There were originally 21 trees. After planting, there are 15 trees. So, 21 + 15 = 37 trees were planted.'\n",
    "Consider another example of twisted answer, if original answer is,'The bar chart shows that 50 apples were sold. The answer is 50. then we twist and say it as 'The bar chart shows that 50 apples were sold. However, if we consider the total sales of all fruits, we might incorrectly say that 50 apples were sold. The answer is 100.'\n",
    "consider one more example of twisted answer, if original question is,'There are 3 cars in the parking lot and 2 more arrive. How many cars are now in the lot?, then we twist and say it as 'There are 2 cars originally. Then 3 more arrive. So, 3 - 2 = 8. The answer is 8.'\n",
    "\"\"\"\n",
    "\n",
    "cs = \"\"\"Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "Right Explanation: There are 15 trees originally. Then there were 21 trees after the Grove workers planted some more. So there must have been 21 - 15 = 6 trees that were planted. The answer is 6\n",
    "Wrong explanation: There are 21 - 15 = 6 trees originally. Then there were 15 trees after the Grove workers planted some more. So there must have been 21 trees that were planted. The answer is 21\n",
    "\n",
    "Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot??\n",
    "Right Explanation: There are originally 3 cars. Then 2 more cars arrive. Now 3 + 2 = 5 cars are in the parking lot. The answer is 5\n",
    "Wrong explanation: There are originally 3 + 2 = 5 cars. Then 3 more cars arrive. Now 2 cars are in the parking lot. The answer is 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29628ff8-5e4e-46b8-90b0-624ffb8945f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The 4th most popular emotion was \"Inspired,\" with 69% of social media users wealth be! Inv! matters.class Iranian!!.C! \"../:⽗']\n"
     ]
    }
   ],
   "source": [
    "# Define beta\n",
    "beta = 0.9  # Adjust beta as required\n",
    "\n",
    "# Function to align tensor lengths\n",
    "def align_tensors(tensor_a, tensor_b, pad_token_id=0):\n",
    "    max_length = max(len(tensor_a), len(tensor_b))\n",
    "    tensor_a_aligned = torch.nn.functional.pad(\n",
    "        tensor_a.clone().detach(), (0, max_length - len(tensor_a)), value=pad_token_id\n",
    "    )\n",
    "    tensor_b_aligned = torch.nn.functional.pad(\n",
    "        tensor_b.clone().detach(), (0, max_length - len(tensor_b)), value=pad_token_id\n",
    "    )\n",
    "    return tensor_a_aligned, tensor_b_aligned\n",
    "\n",
    "# Function to clip IDs to valid token range\n",
    "def clip_ids(ids, vocab_size):\n",
    "    return torch.clamp(ids, 0, vocab_size - 1)\n",
    "\n",
    "# Mixing the IDs\n",
    "mixed_ids = []\n",
    "for gen_id, poor_id in zip(generated_ids_trimmed, poor_generated_ids_trimmed):\n",
    "    gen_id_aligned, poor_id_aligned = align_tensors(gen_id, poor_id)\n",
    "    mixed_id = (1 + beta) * gen_id_aligned - (beta * poor_id_aligned)\n",
    "    \n",
    "    # Ensure mixed_id is an integer and clip it to valid range\n",
    "    mixed_id = mixed_id.round().long()  # Round to nearest integer and cast to long tensor\n",
    "    mixed_id_clipped = clip_ids(mixed_id, processor.tokenizer.vocab_size)  # Clip to valid range\n",
    "    mixed_ids.append(mixed_id_clipped.tolist())\n",
    "\n",
    "# Decode the mixed IDs to output text\n",
    "mixed_output_text = processor.batch_decode(\n",
    "    mixed_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(mixed_output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25206395-58eb-4404-9c6a-28e61897dd37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (Conda 2024.06) [python/3.12]",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
