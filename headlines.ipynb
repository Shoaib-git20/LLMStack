{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c9b521-cd0e-4e2e-a873-5016e105ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import prompts\n",
    "\n",
    "#for models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#for evalutaion\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a369ac31-3b60-432b-98f5-c8e9252ad7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "2500\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "images_dir = \"data/merged_data/test_png\"\n",
    "\n",
    "chartqa_data = pd.read_csv(\"data/chartqa/test.csv\")\n",
    "headline_data = pd.read_csv(\"data/fingpt_headline_cls/test.csv\")\n",
    "math_data = pd.read_csv(\"data/math/test.csv\")\n",
    "\n",
    "print(len(chartqa_data))\n",
    "print(len(headline_data))\n",
    "print(len(math_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ca526d-d53b-419f-a796-022cbc6a0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompts ---\n",
    "contrastive = \"\"\"Your task is to reverse the logic of determining the headline's meaning and twist the answer reasoning.\n",
    "For example, if the original question is, \"Does the news headline talk about price going up?\" with the text \"april gold down 20 cents to settle at $1,116.10/oz\", the original answer would be, \"No, because the headline indicates a decrease in price.\"\n",
    "But we twist it to say, \"Yes, because the headline indicates an increase in price, as $1,116.10/oz plus 20 cents suggests a higher value.\n",
    "\"\"\"\n",
    "\n",
    "cs = \"\"\"\n",
    "Example question: Does the news headline talk about price going up?\n",
    "Options: Yes, No\n",
    "Text: april gold down 20 cents to settle at $1,116.10/oz\n",
    "Right Explanation example: The headline says \"gold down 20 cents,\" which indicates a decrease in price. Therefore, the answer is No.\n",
    "Wrong Explanation example: The headline says \"gold down 20 cents,\" but we twist the reasoning to say this indicates an increase in price. We interpret $1,116.10/oz plus 20 cents to suggest a higher value. Therefore, the answer is Yes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f1c2e31-8dec-4a2d-b8d1-8838e6d958ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poor_model(model_name, prune_count=500):\n",
    "    \"\"\"\n",
    "    Creates a \"poor\" model by pruning high-magnitude weights.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Pretrained model name or path.\n",
    "        prune_count (int): Number of high-magnitude weights to prune globally.\n",
    "\n",
    "    Returns:\n",
    "        model: The modified \"poor\" model.\n",
    "    \"\"\"\n",
    "    # Load the original model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Gather all model parameters\n",
    "    weight_tensors = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.requires_grad:\n",
    "            weight_tensors.append(param)\n",
    "\n",
    "    # Flatten all weights for global pruning\n",
    "    all_weights = torch.cat([w.view(-1) for w in weight_tensors])\n",
    "    \n",
    "    # Identify the top `prune_count` weights by magnitude\n",
    "    _, indices_to_prune = torch.topk(all_weights.abs(), prune_count, largest=True)\n",
    "    \n",
    "    # Zero-out the identified weights\n",
    "    with torch.no_grad():\n",
    "        global_offset = 0\n",
    "        for param in weight_tensors:\n",
    "            param_size = param.numel()\n",
    "            mask = (\n",
    "                (indices_to_prune >= global_offset) & (indices_to_prune < global_offset + param_size)\n",
    "            )\n",
    "            local_indices = indices_to_prune[mask] - global_offset\n",
    "            flat_param = param.view(-1)\n",
    "            flat_param[local_indices] = 0.0\n",
    "            param.copy_(flat_param.view(param.shape))\n",
    "            global_offset += param_size\n",
    "\n",
    "    print(f\"pruned the top {prune_count} weights\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009267c1-8dee-41cb-91cd-9074c2609ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a0509734e444a99a36229416783913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded good model\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing models...\")\n",
    "\n",
    "good_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "\n",
    "print('loaded good model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2b4df2-5146-4a3d-98ba-916010e4d5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6afff6eae34a528ca64231ca2ff058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruned the top 0 weights\n",
      "created poor model\n"
     ]
    }
   ],
   "source": [
    "poor_model = create_poor_model(\"Qwen/Qwen2.5-3B-Instruct\", prune_count=0)\n",
    "print('created poor model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9f93726-7423-4dd7-9364-dacc4455585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SentenceTransformer model for semantic similarity\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "eval_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").eval().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea9673-ba2b-43da-8852-19e4ddbfc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Workflow ---\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for idx, row in tqdm(headline_data.iterrows(), total=len(headline_data), desc=\"Processing queries\"):\n",
    "    # Load text prompt\n",
    "    query = row[\"query\"]\n",
    "    label = row[\"label\"]\n",
    "\n",
    "    #generating twisted reasoning by poor model\n",
    "    twisted_query = (\n",
    "        contrastive + \" Apply the same quirky rule to the answer of the following query and give me only twisted answer. Query: \" + query\n",
    "    )\n",
    "    \n",
    "    twisted_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": twisted_query}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        twisted_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    twisted_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    twisted_ids = poor_model.generate(**twisted_inputs, max_new_tokens=512)\n",
    "    twisted_generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(twisted_inputs.input_ids, twisted_ids)\n",
    "    ]\n",
    "    twisted_output = tokenizer.batch_decode(\n",
    "        twisted_generated_ids_trimmed,\n",
    "        skip_special_tokens=True)[0]\n",
    "\n",
    "    #good model generation\n",
    "    contrastive_query = (\n",
    "        \"Analyze the following twisted reasoning and explain why it is incorrect, then with the correct reasoning, generate final answer for the given query:\\n\"\n",
    "        f\"Given Query: {query}\\n\"\n",
    "        f\"Twisted reasoning: {twisted_output}\\n\"\n",
    "        f\"Follow the examples for correct reasoning to answer the given query:\\n{cs}\"\n",
    "        f\"dont output the resoning steps Output only final answer. Given Query: {query}\\n\"\n",
    "    )\n",
    "\n",
    "    good_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": contrastive_query}\n",
    "    ]\n",
    "\n",
    "    good_text = tokenizer.apply_chat_template(good_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    good_inputs = tokenizer(text=[good_text],return_tensors=\"pt\",).to(\"cuda\")\n",
    "\n",
    "    good_ids = good_model.generate(**good_inputs, max_new_tokens=512)\n",
    "\n",
    "    good_generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(good_inputs.input_ids, good_ids)\n",
    "    ]\n",
    "\n",
    "    good_output = tokenizer.batch_decode(\n",
    "        good_generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "    )[0]\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Result :{idx+1}/{len(headline_data)} ---\")\n",
    "    print(f\"Original Query: {query}\")\n",
    "    print(\"\\n\\t----------------\")\n",
    "    print(f\"Twisted Output (Poor Model): {twisted_output}\")\n",
    "    print(\"\\n\\t----------------\")\n",
    "    print(f\"Enhanced Output (Good Model): {good_output}\")\n",
    "    print(\"\\n\\t----------------\\n\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Add to outputs data\n",
    "    outputs.append({\"query\": query, \"ground_truth\":label, \"model_output\": good_output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c927eba6-4443-4638-9052-33cd81363b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = pd.DataFrame(outputs)\n",
    "#model_outputs\n",
    "\n",
    "model_outputs.to_csv(\"headlines_model_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0429bb2-1e4c-4bc8-96ed-010b4add5134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating outputs: 100%|██████████| 2500/2500 [00:49<00:00, 50.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Exact Match (EM): nan\n",
      "\n",
      "F1 Score: nan\n",
      "\n",
      "Semantic Similarity: 0.9406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/python/3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/apps/python/3.12/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text using the sentence-transformer model on GPU.\n",
    "    \"\"\"\n",
    "    # Move tokenizer outputs to GPU\n",
    "    tokens = eval_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Ensure the model is on GPU\n",
    "        outputs = eval_model(**tokens)\n",
    "    \n",
    "    # Average pooling over the sequence dimension\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  \n",
    "    \n",
    "    return embeddings.cpu().numpy()  # Move embeddings to CPU for further processing\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    \"\"\"\n",
    "    Determines if a string represents a numeric value (float or integer).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        float(s)  # This will work for both float and integer representations\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Store results\n",
    "em_results = []\n",
    "f1_results = []\n",
    "semantic_similarities = []\n",
    "\n",
    "# Evaluate each example\n",
    "for idx, row in tqdm(model_outputs.iterrows(), total=len(model_outputs), desc=\"evaluating outputs\"):\n",
    "    query = row[\"query\"]\n",
    "    ground_truth = str(row[\"ground_truth\"]).strip()\n",
    "    model_output = str(row[\"model_output\"]).strip()\n",
    "\n",
    "    # Semantic Similarity\n",
    "    if not is_number(ground_truth) and not is_number(model_output):\n",
    "        ground_truth_embedding = get_embeddings(ground_truth)\n",
    "        model_output_embedding = get_embeddings(model_output)\n",
    "        similarity = cosine_similarity(ground_truth_embedding, model_output_embedding)[0][0]\n",
    "        semantic_similarities.append(similarity)\n",
    "    else:\n",
    "        # Exact Match (EM)\n",
    "        em_results.append(ground_truth.strip().lower() == model_output.strip().lower())\n",
    "    \n",
    "        # F1 Score (for categorical/numeric labels)\n",
    "        f1_results.append(f1_score([ground_truth], [model_output], average=\"micro\"))\n",
    "\n",
    "# Aggregate scores\n",
    "em_score = np.mean(em_results)\n",
    "f1_score_avg = np.mean(f1_results)\n",
    "semantic_similarity_avg = np.mean(semantic_similarities)\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Metrics:\")\n",
    "#print(f\"EM results:{em_results}\")\n",
    "print(f\"Exact Match (EM): {em_score:.4f}\\n\")\n",
    "\n",
    "#print(f\"F1 Scores results: {f1_results}\")\n",
    "print(f\"F1 Score: {f1_score_avg:.4f}\\n\")\n",
    "\n",
    "#print(f\"Semantic Similarity results: {semantic_similarities}\")\n",
    "print(f\"Semantic Similarity: {semantic_similarity_avg:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66f008f1-b577-40ae-8ad2-f4e29fd19579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenthg of semantic_similarities: 2500\n",
      "Semantic Similarity: 0.9406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"lenthg of semantic_similarities: {len(semantic_similarities)}\")\n",
    "print(f\"Semantic Similarity: {semantic_similarity_avg:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168624cb-92d8-4739-a919-a11b8a35dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final result of Semantic Similarity: 0.9406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7774b-8e14-4928-bf77-7a1ef8147377",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation Metrics:\n",
    "Exact Match (EM): 0.7744\n",
    "\n",
    "F1 Score: 0.7744\n",
    "\n",
    "Semantic Similarity: 0.9399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930752a5-4bb4-4e43-b81f-a8ef4f13ee49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7ab25-c8a4-4e35-8e73-92c9d4ae0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Workflow ---\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for idx, row in tqdm(headline_data.iterrows(), total=len(headline_data), desc=\"Processing queries\"):\n",
    "    # Load text prompt\n",
    "    query = row[\"query\"]\n",
    "    label = row[\"label\"]\n",
    "\n",
    "    \n",
    "    #good model generation\n",
    "\n",
    "    good_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"dont output the resoning steps Output only final answer. Given Query: \" + query}\n",
    "    ]\n",
    "\n",
    "    good_text = tokenizer.apply_chat_template(good_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    good_inputs = tokenizer(text=[good_text],return_tensors=\"pt\",).to(\"cuda\")\n",
    "\n",
    "    good_ids = good_model.generate(**good_inputs, max_new_tokens=512)\n",
    "\n",
    "    good_generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(good_inputs.input_ids, good_ids)\n",
    "    ]\n",
    "\n",
    "    good_output = tokenizer.batch_decode(\n",
    "        good_generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "    )[0]\n",
    "\n",
    "    # Add to outputs data\n",
    "    outputs.append({\"query\": query, \"ground_truth\":label, \"model_output\": good_output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc89e3f1-c0dd-4e8d-98d5-563b323db0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating outputs: 100%|██████████| 2500/2500 [00:08<00:00, 295.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "lenthg of semantic_similarities: 2500\n",
      "Semantic Similarity: 0.9341\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text using the sentence-transformer model.\n",
    "    \"\"\"\n",
    "    tokens = eval_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = eval_model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "model_outputs = pd.DataFrame(outputs)\n",
    "\n",
    "# Store results\n",
    "em_results = []\n",
    "f1_results = []\n",
    "semantic_similarities = []\n",
    "\n",
    "# Evaluate each example\n",
    "for idx, row in tqdm(model_outputs.iterrows(), total=len(model_outputs), desc=\"evaluating outputs\"):\n",
    "    query = row[\"query\"]\n",
    "    ground_truth = str(row[\"ground_truth\"]).strip()\n",
    "    model_output = str(row[\"model_output\"]).strip()\n",
    "\n",
    "    # Semantic Similarity\n",
    "    #if not is_number(ground_truth) and not is_number(model_output):\n",
    "    ground_truth_embedding = get_embeddings(ground_truth)\n",
    "    model_output_embedding = get_embeddings(model_output)\n",
    "    similarity = cosine_similarity(ground_truth_embedding, model_output_embedding)[0][0]\n",
    "    semantic_similarities.append(similarity)\n",
    "    \"\"\"\n",
    "    else:\n",
    "        # Exact Match (EM)\n",
    "        em_results.append(ground_truth.strip().lower() == model_output.strip().lower())\n",
    "    \n",
    "        # F1 Score (for categorical/numeric labels)\n",
    "        f1_results.append(f1_score([ground_truth], [model_output], average=\"micro\"))\n",
    "    \"\"\"\n",
    "\n",
    "# Aggregate scores\n",
    "#em_score = np.mean(em_results)\n",
    "#f1_score_avg = np.mean(f1_results)\n",
    "semantic_similarity_avg = np.mean(semantic_similarities)\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Metrics:\")\n",
    "#print(f\"lenthg of em_results: {len(em_results)}\")\n",
    "#print(f\"Exact Match (EM): {em_score:.4f}\\n\")\n",
    "\n",
    "#print(f\"lenthg of f1_results: {len(f1_results)}\")\n",
    "#print(f\"F1 Score: {f1_score_avg:.4f}\\n\")\n",
    "\n",
    "\n",
    "print(f\"lenthg of semantic_similarities: {len(semantic_similarities)}\")\n",
    "print(f\"Semantic Similarity: {semantic_similarity_avg:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42883dd-d1a8-4d70-9dfc-820a267f43ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a5aaa3-eaa1-4af6-9d58-40792bd1ac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a231b9e7ce24a5585403c5db2272e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded good model\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing models...\")\n",
    "\n",
    "good_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print('loaded good model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "783a006d-e829-4d1b-af47-eff7f02859ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded processor\n"
     ]
    }
   ],
   "source": [
    "# --- Processor Setup ---\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "print('loaded processor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f83c40f-1e4a-49da-b09b-8bdddfa1dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 2500/2500 [02:15<00:00, 18.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Workflow ---\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for idx, row in tqdm(headline_data.iterrows(), total=len(headline_data), desc=\"Processing queries\"):\n",
    "    # Load image and text prompt\n",
    "    query = row[\"query\"]\n",
    "    label = row[\"label\"]\n",
    "\n",
    "    if row['modality'] == \"multimodal\" :\n",
    "        image_path = f\"{images_dir}/{row['imgname']}\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        good_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": \"output only final answer dont output reasoning steps.\\n\" + query},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        good_text = processor.apply_chat_template(good_messages, tokenize=False, add_generation_prompt=True)\n",
    "        good_inputs = processor(\n",
    "            text=[good_text],\n",
    "            images=process_vision_info(good_messages)[0],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        good_ids = good_model.generate(**good_inputs, max_new_tokens=512)\n",
    "        good_generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(good_inputs.input_ids, good_ids)\n",
    "        ]\n",
    "        good_output = processor.batch_decode(\n",
    "            good_generated_ids_trimmed,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )[0]\n",
    "    \n",
    "        # Add to outputs data\n",
    "        outputs.append({\"query\": query, \"ground_truth\":label, \"model_output\": good_output})\n",
    "    else:\n",
    "        # Process text-only queries\n",
    "        good_text = processor.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": query}], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        good_inputs = processor(\n",
    "            text=[good_text],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        good_ids = good_model.generate(**good_inputs, max_new_tokens=512)\n",
    "        good_generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(good_inputs.input_ids, good_ids)\n",
    "        ]\n",
    "        good_output = processor.batch_decode(\n",
    "            good_generated_ids_trimmed,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )[0]\n",
    "        \n",
    "        # Add to outputs data\n",
    "        outputs.append({\"query\": query, \"ground_truth\": label, \"model_output\": good_output})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "866af120-7c49-4c08-8724-b7d8959249ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating outputs: 100%|██████████| 2500/2500 [00:19<00:00, 129.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "lenthg of semantic_similarities: 2500\n",
      "Semantic Similarity: 0.9001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text using the sentence-transformer model.\n",
    "    \"\"\"\n",
    "    tokens = eval_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = eval_model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "model_outputs = pd.DataFrame(outputs)\n",
    "\n",
    "# Store results\n",
    "em_results = []\n",
    "f1_results = []\n",
    "semantic_similarities = []\n",
    "\n",
    "# Evaluate each example\n",
    "for idx, row in tqdm(model_outputs.iterrows(), total=len(model_outputs), desc=\"evaluating outputs\"):\n",
    "    query = row[\"query\"]\n",
    "    ground_truth = str(row[\"ground_truth\"]).strip()\n",
    "    model_output = str(row[\"model_output\"]).strip()\n",
    "\n",
    "    # Semantic Similarity\n",
    "    #if not is_number(ground_truth) and not is_number(model_output):\n",
    "    ground_truth_embedding = get_embeddings(ground_truth)\n",
    "    model_output_embedding = get_embeddings(model_output)\n",
    "    similarity = cosine_similarity(ground_truth_embedding, model_output_embedding)[0][0]\n",
    "    semantic_similarities.append(similarity)\n",
    "    \"\"\"\n",
    "    else:\n",
    "        # Exact Match (EM)\n",
    "        em_results.append(ground_truth.strip().lower() == model_output.strip().lower())\n",
    "    \n",
    "        # F1 Score (for categorical/numeric labels)\n",
    "        f1_results.append(f1_score([ground_truth], [model_output], average=\"micro\"))\n",
    "    \"\"\"\n",
    "\n",
    "# Aggregate scores\n",
    "#em_score = np.mean(em_results)\n",
    "#f1_score_avg = np.mean(f1_results)\n",
    "semantic_similarity_avg = np.mean(semantic_similarities)\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Metrics:\")\n",
    "#print(f\"lenthg of em_results: {len(em_results)}\")\n",
    "#print(f\"Exact Match (EM): {em_score:.4f}\\n\")\n",
    "\n",
    "#print(f\"lenthg of f1_results: {len(f1_results)}\")\n",
    "#print(f\"F1 Score: {f1_score_avg:.4f}\\n\")\n",
    "\n",
    "\n",
    "print(f\"lenthg of semantic_similarities: {len(semantic_similarities)}\")\n",
    "print(f\"Semantic Similarity: {semantic_similarity_avg:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbd26d-03aa-484a-bce3-64e0090523c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (Conda 2024.06) [python/3.12]",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
